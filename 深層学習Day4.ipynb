{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "深層学習Day4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlpBNL5p1X64iaipW/GQRU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lake198/Study-AI/blob/main/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92Day4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Section1] 強化学習"
      ],
      "metadata": {
        "id": "gfF5K9JV6Hxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "強化学習は機械学習の一種である。教師有り・教師なし学習が与えられたデータの特徴を学習するのに対し、強化学習は環境から与えられる報酬を目的に行動するエージェントの行動指針 = 方策を学習する。\n",
        "## 方策関数・価値関数\n",
        "環境がある状態 $s$ にある時に、エージェントが行動 $a$ を取る確率を出力する関数を方策関数と呼ぶ。またエージェントが方策 $\\pi$ に基づいて行動した時、将来得られる価値を出力する関数を価値関数と呼ぶ。価値関数は環境価値関数と行動価値関数がある。環境価値関数は、環境 $s$ の下で方策 $\\pi$ を続けた時に将来得られる報酬の期待値を出力する（エージェントが取る行動には依存しない）。$V^{\\pi}(s)$ と表記する。行動価値関数は、環境 $s$ の下で行動 $a$ を取り、方策 $\\pi$ を続けた場合に将来得られる報酬の期待値を出力する。$Q^{\\pi}(a,s)$ と表記する。\n",
        "## 方策勾配法\n",
        "方策勾配法は方策 $\\pi$ をモデル化して最適化する。方策を決定するパラメータとして $\\theta$ を導入し、方策関数を $\\pi(a|s,\\theta)$ で定義する。また得られる報酬を最大化するための目的関数 $J(\\theta)$ を導入し、その勾配を使って $\\theta$ を更新する。\\\n",
        "\\\n",
        "$$\n",
        "\\theta \\leftarrow \\theta + ϵ\\nabla J(\\theta)\n",
        "$$\\\n",
        "$J(\\theta)$ の勾配は方策勾配定理を用いて次式で計算される。\\\n",
        "\\\n",
        "$$\n",
        "\\nabla J(\\theta)=E\\left[ \\nabla_{\\theta} \\rm{log} \\pi_\\theta(a|s)Q^\\pi(a,s)\\right]\n",
        "$$\n"
      ],
      "metadata": {
        "id": "0_cXQmQA6liL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Section 2] Alpha Go"
      ],
      "metadata": {
        "id": "a1Dkqfkm6MU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Alpha Go Lee\n",
        "Policy NetとValue Netの2つのネットワークを学習する。PolicyNetが方策関数、Value Net が行動価値関数であり、両者とも畳み込みNNである。Policy Netは19×19（碁盤のマス目）× 48 ch の入力を持ち、着手予想確率を出力する。Value Net は 19×19×49 の入力を持ち、現局面の勝率を出力する。\\\n",
        "　Alpha Go の学習においては、学習を効率的に進めるために 上記ネットワークに加えてRole Out Policy を用意し、強化学習に先んじてRole out Policy と Polici Net の教師あり学習を行っている。ここで、Role Out policy はPolicy Netよりも簡易的かつ高速な着手予想モデルである。\\\n",
        "　これらの教師あり学習を行ったのち、次にPolicy Netの強化学習を行う。この学習は現状のPolicy Netと過去の学習過程であるPolicy Poolからランダムに選ばれたPolici Netとの対局結果を用いて、方策勾配法により行っている。\\\n",
        "　続いてPolicy Netを用いた対局シミュレーションを行い、その勝敗を教師として Value Net の学習を行ている。囲碁では各局面での盤面の価値を評価することが難しいため、末端の勝敗結果のみを使って探索を行うために、モンテカルロ木探索が用いられている。\n",
        "##Alpha Go Zero\n",
        "Alpha Go Zero はAlpha Go Lee と異なり、教師あり学習を行わず強化学習のみで作成している。また入力する特徴を石の配置のみとして、入力データのチャネル数を削減している。さらにPolicy Net と Value Net を一つのネットワークに統合し、途中で枝分かれさえる事でPolicyとValueを出力している。またAlpha Go Zero は2つの畳み込み層を持つResidual Block を重ね合わせた Residual Net を導入している。NNの層が深くなると勾配消失問題が発生するが、Residual Blockは層をスキップするショートカットを持つため、これを回避することができる。 "
      ],
      "metadata": {
        "id": "rjoOukZLqITG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Section 3] 軽量化・高速化技術"
      ],
      "metadata": {
        "id": "5bcDPuah6SQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 分散深層学習\n",
        "現在、機械学習モデルのはコンピューター性能の向上スピードに比べてはるかに速いスピードで複雑さが増している。そこで計算資源（ワーカー）を複数用意して、並列的にニューラルネットを構成しての学習が行われる。\n",
        "\n",
        "## データ並列化\n",
        "親モデルを各ワーカーに子モデルとしてコピーし、入力データを分割して各ワーカーに学習させる。学習の進め方は同期型と非同期型がある。同期型は全てのワーカーの学習が終わるのを待ち、各ワーカーの勾配を平均して親モデルの勾配を更新する。非同期型は各ワーカーは自分の学習が終わったら更新した勾配をパラメータサーバーにPushする。次に学習が終了したワーカーはパラメータサーバー上の学習済みモデルを利用して、新たな学習を開始する。\n",
        "\n",
        "## モデル並列化\n",
        "親モデルを複数に分割して各ワーカで学習を行う。直列のモデルを分割する場合もあれが、枝分かれした並列構造を分割する場合が多い。\n",
        "\n",
        "## GPU\n",
        "CPUが少数の高性能コアで構成されるのに対し、GPUは比較的低性能なコアを多数組み合わせて構成される。簡単な処理を並列化して高速に行う事が可能なので、行列計算を多数行うNNの学習に適する。現在はDeep Learning 用のGPU計算プラットフォームとして NVIDIA が提供する CUDA がよく用いられる。\\\n",
        "\n",
        "## 量子化\n",
        "パラメータのbit数を小さくすることで、メモリの使用量を削減し、演算を高速化することができる。現在のGPU性能は半精度(16bit)で150 Tera Flops 程度である。計算速度と計算精度はトレードオフの関係があるが、実際の面では高速化のメリットの方が大きい場合が多い。\n",
        "\n",
        "## 蒸留\n",
        "精度の高いモデルはニューロンの数が多く、推論時に多くのメモリと演算処理を要する。そこで精度の高い大規模モデルを教師モデルとして、より軽量な生徒モデルを作成する。学習においては教師モデルの重みは固定とし、教師モデルと生徒モデルの両方の誤差を使って生徒モデルの重みを更新する。\n",
        "\n",
        "## プルーニング\n",
        "大規模なニュラルネットワークにおいて、予測結果の精度への影響が小さいニューロンを削減することで、モデルを軽量化し計算を高速化することができる。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DJwbbM90CV4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Section 4] 応用モデル"
      ],
      "metadata": {
        "id": "7uxEfVMU6XMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mobile Nets\n",
        "通常の畳み込み演算において、H×W×C の入力データを K×K×C のフィルタで畳み込んでチャネル数 M の出力を得る場合、その計算量は H×W×K×K×C×M となる。Mobile Nets は以下で説明する Depthwise Convolution と Pointwise Convolution を組み合わせる事で計算量を削減する。\\\n",
        "\\\n",
        "Depthwise Convolution\\\n",
        "サイズが K×K×1 のフィルタを用いて入力データのチャネル毎に畳み込みを行う。出力チャネル数は入力チャネル数と同じになる。計算量は H×W×K×K×C となる。\\\n",
        "\\\n",
        "Pointwise Convolution\\\n",
        "サイズが 1×1×M のフィルタを用いて畳み込みを行う。出力チャネルはH×W×M のデータとなる。計算量は H×W×C×M となる。\\\n",
        "\\\n",
        "Mobile Nets は上記2つの畳み込みを組み合わせて出力マップを得るため、計算量は H×W×K×K×C + H×W×C×M となり、通常の畳み込み演算に比べて軽量化が実現できる。\n",
        "\n",
        "## Dense Net\n",
        "Dense Netは層が深くなった際の勾配消失問題を解決するために考案されたCNNの一種である。Dense Net は複数の畳み込み層を持つ Dense Block と Dense Block どうしをつなぐ Transition Layer で構成される。Dense Blockでは畳み込み層を通過する事に、前の層の出力を新たなチャネルとして足し合わせる。追加するチャネルの数は growth rate と呼ばれるハイパーパラメータである。Transition Layer では Dense Block で増加したチャネル数の圧縮とプーリング層によるダウンサンプリングが行われる。\n",
        "\n",
        "## Batch Normalization\n",
        "与えられた入力データをミニバッチ単位で平均 0、分散 1 となるように変換する。ミニバッチに含まれるデータの各チャネルが一つの分布に従う。バッチサイズは計算機の性能によって変える必要があるため、正規化の効果を計りにくい。またバッチサイズが小さい場合は効果が薄くなってしまう。\n",
        "\n",
        "## Layer Normalization\n",
        "与えられた入力データをチャネル単位で正規化する。各データの全てのピクセルが一つの分布に従う。Layer Norm.は入力データのスケーリングや、重み行列のスケーリングおよびシフトに対してロバストである事が知られている。\n",
        "\n",
        "## Wave Net\n",
        "与えられた音声データを1次元のつながりを持ったデータとして取り扱い、畳み込みNNによって音声波形を生成する。この時、通常のCNNでは隣り合うデータどうしを畳み込むが、Wave Netでは層が深くなるにつれてデータの間隔を離して畳み込みを行う Dilated convolution が用いられる。この方法によってパラメータに対する受容野を簡単に増やすことができる。\n"
      ],
      "metadata": {
        "id": "yCAOM7aDRua3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Section 5] Transformer"
      ],
      "metadata": {
        "id": "sekIWfsL6bww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TransformerはSeq2Seqと同様に機械翻訳で良く使われる Encoder-Decoder モデルであるが、RNN を用いずにAttention機構のみを用いる点が Seq2Seq とは異なる。RNNによる逐次処理を行わないので、計算量が非常に少なくなる。\n",
        "\n",
        "## Encoder\n",
        "Transformer における Encoder の主要モジュールは Multi-head attention 機構とFeed Foward NN である。ただし、Transformer はRNN が無いため、Embeddingした入力データに対して Position Encoding を行う。Position Encoding は単語の分散ベクトルに sin 関数と cos 関数のパターンを用いて単語の相対的な位置情報を加算する。\\\n",
        "Encoder では入力データに対して Self-Attention を行う。Self-Attention は query, key, value に全て同じテンソルを用いる。またTransformerではSeq2SeqのAttentionのような逐次処理は行わず、全ての時系列データに対するAttention Weight を一括で計算する。\\\n",
        "\\\n",
        "$$\n",
        "Attention(Q,K,V)=softmax\\left( \\frac{QK^T}{\\sqrt{d_k}}V \\right)\n",
        "$$\\\n",
        "ここで $d_k$ は単語の分散表現ベクトルの次元数である。内積計算で大きすぎる値があると、softmax 関数によってその他の要素が0となり、勾配が消失してしまうため、スケーリングを行って値を小さくしている。\\\n",
        "Transformer のAttention機構はMulti head Attention となっている。入力された query, key, value を複数の組（Head）に分割し、各組についてそれぞれ Attention の計算を行う。一つの大きな Head で計算するよりも、複数の小さな Head で計算を行い、最後に concat する方がパフォーマンスが上がると言われている。\\\n",
        "Attentionからの出力を2層の全結合層である Feed Foward NN に入力する。ここでは単語の位置情報を保持したまま順伝搬を行い、Encodrの出力を得る。\n",
        "\n",
        "## Decoder\n",
        "DecoderはEncoderと類似した構成となっているが、Self-Attention 機構の後ろに Source-Target Attention 機構を持ち、Encoder の出力に対する Attention を計算している。また Attention の計算は全ての単語に対して一括で行うため、Decoderにおいては未来の結果を参照しないように Mask 処理を行っている。Attention の出力をFeed Foward NN で変換し、最後に softmax 関数によってトークンの生成確率に変換して、最終的な文章を得る。 \n"
      ],
      "metadata": {
        "id": "5jLVzbURdLjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Section 6] 物体検知・セグメンテーション"
      ],
      "metadata": {
        "id": "h7f763aB6f4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "画像認識のタスクは主に下記の4つに分類される。\\\n",
        "・分類問題 (Classification)\\\n",
        "・物体検知 (Object detection)\\\n",
        "　　物体の位置を Bounding Box で囲む。\\\n",
        "・意味領域分割 (semantic segmentation)\\\n",
        "　　各ピクセルにクラスラベルを割り当てる。\\\n",
        "・個体領域分割 (Instance segmentation)\\\n",
        "　　各ピクセルにクラスラベルを割り当て、さらに各インスタンスを区別する。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# データセット\n",
        "　画像認識の学習や精度評価においては、目的に応じて適切なデータセットを選択する必要がある。クラス数、データ数（学習用＋評価用）に加えて、一枚の画像あたりに対象が含まれる数 (Box/画像) も考慮に入れるのが良い。Box/画像 が小さいとアイコン的な映りとなる。大きいと対象が小さくなったり部分的な重なりが発生する傾向がある。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 物体検出における評価指標\n",
        "物体検出においては、クラスラベルの正誤だけではなく、物体位置の予測精度も評価したいので、評価指標にIoU (Intersection of Union) を用いる。\\\n",
        "\\\n",
        "$$\n",
        "\\mbox{IoU}=\\frac{\\mbox{Area of Overlap}}{\\mbox{Area of Union}}\n",
        "$$\\\n",
        "ここで、以下は評価指標として誤りであることに注意する。\\\n",
        "\\\n",
        "$$\n",
        "\\mbox{IoU}=\\frac{\\mbox{Area of Overlap}}{\\mbox{Area of Grand Truth BB}}, \\quad\n",
        "\\mbox{IoU}=\\frac{\\mbox{Area of Overlap}}{\\mbox{Area of Prediction}}\n",
        "$$\\\n",
        "物体検知における Presision 、Recall の評価は、confidence（以下 conf.）とIOU のそれぞれの閾値に対して、$\\rm conf.>th_{conf}$ かつ $\\rm IoU>th_{IoU}$ の時 TP (True Positive) となる。ここで、検出対象に対してより高い conf. で既に TP と判定されている場合は、条件を満たしている場合でも FP となる事に注意する。\n",
        "ある一つのクラスラベルに対して、$\\rm th_{IoU}$ を固定し、$\\rm th_{conf.}$ を0～1 の範囲で変化させると、Recall に対する Presision の変化を表す PR 曲線が描ける。この曲線を Recall = 0～1 の範囲で積分した値を Average Presision : AP と呼ぶ。また全ての分類クラスの AP を平均したものを maan Average Presision : mAP と呼ぶ。1\\\n",
        "\\\n",
        "$$\n",
        "\\rm{AP} = \\int_{0}^{1}P(R)dR\\\\\n",
        "\\rm{mAP} = \\frac{1}{C}\\sum_{i=1}^CAP_i\n",
        "$$\\\n",
        "物体検出のフレームワークは様々なものがあるが、大きくは1段階の検出を行うものと2段階の検出を行うもので分類される。1段階検出器は物体位置の検出と分類を一つのネットワークで行うが、2段階検出器は最初に位置を検出し、そこで抽出した画像に対して分類を行う。相対的に1段階検出器は検出速度に勝り、2段階検出器は検出精度に勝る。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# SSD (Single Shot Detector)\n",
        "SSDは1段階検出器に分類される物体検出用フレームワークであり、VGG16 CNN を基本構造としている。SSDはサイズの異なる複数の畳み込み層から特徴マップを出力する。各特徴マップは一つの特徴量毎にBounding Box (Default Box) を持ち、一つのDefault Boxは クラス数+4 の結果を出力する（4 はBBの位置を表す指標）。特徴マップは層が深くなるほど解像度が低くなるので、浅い層のBBは比較的小さな物体を検出し、深い層のBBは大きな物体を検出する。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Semantic Segmentation\n",
        "Semantic Secmentation においては、入力画像の各ピクセル毎に分類を行うため、畳み込みによって得られた特徴マップを入力画像と同じ解像度に Upsampling する必要がある。この工程を Deconvolution と呼ぶ。ここで注意すべき点として、Pooling 処理によって失われたローカルな情報が Deconvolution によって復元するわけではない。そこで低レイヤーの畳み込み層で出力された特徴マップを Upsampling した特徴マップに要素ごとに足し合わせて、失われた輪郭情報を補完することが行われる。\n"
      ],
      "metadata": {
        "id": "Y1DGgJ2TrOq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 実装演習 Transformer"
      ],
      "metadata": {
        "id": "Dv8vGJ3nKxN3"
      }
    },
    {
      "metadata": {
        "id": "9T8VxiHfp2bG"
      },
      "cell_type": "code",
      "source": [
        "def position_encoding_init(n_position, d_pos_vec):\n",
        "    \"\"\"\n",
        "    Positional Encodingのための行列の初期化を行う\n",
        "    :param n_position: int, 系列長\n",
        "    :param d_pos_vec: int, 隠れ層の次元数\n",
        "    :return torch.tensor, size=(n_position, d_pos_vec)\n",
        "    \"\"\"\n",
        "    # PADがある単語の位置はpos=0にしておき、position_encも0にする\n",
        "    position_enc = np.array([\n",
        "        [pos / np.power(10000, 2 * (j // 2) / d_pos_vec) for j in range(d_pos_vec)]\n",
        "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n",
        "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2])  # dim 2i\n",
        "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2])  # dim 2i+1\n",
        "    return torch.tensor(position_enc, dtype=torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N7FawizWp2ba"
      },
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, attn_dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param d_model: int, 隠れ層の次元数\n",
        "        :param attn_dropout: float, ドロップアウト率\n",
        "        \"\"\"\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.temper = np.power(d_model, 0.5)  # スケーリング因子\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, q, k, v, attn_mask):\n",
        "        \"\"\"\n",
        "        :param q: torch.tensor, queryベクトル, \n",
        "            size=(n_head*batch_size, len_q, d_model/n_head)\n",
        "        :param k: torch.tensor, key, \n",
        "            size=(n_head*batch_size, len_k, d_model/n_head)\n",
        "        :param v: torch.tensor, valueベクトル, \n",
        "            size=(n_head*batch_size, len_v, d_model/n_head)\n",
        "        :param attn_mask: torch.tensor, Attentionに適用するマスク, \n",
        "            size=(n_head*batch_size, len_q, len_k)\n",
        "        :return output: 出力ベクトル, \n",
        "            size=(n_head*batch_size, len_q, d_model/n_head)\n",
        "        :return attn: Attention\n",
        "            size=(n_head*batch_size, len_q, len_k)\n",
        "        \"\"\"\n",
        "        # QとKの内積でAttentionの重みを求め、スケーリングする\n",
        "        attn = torch.bmm(q, k.transpose(1, 2)) / self.temper  # (n_head*batch_size, len_q, len_k)\n",
        "        # Attentionをかけたくない部分がある場合は、その部分を負の無限大に飛ばしてSoftmaxの値が0になるようにする\n",
        "        attn.data.masked_fill_(attn_mask, -float('inf'))\n",
        "        \n",
        "        attn = self.softmax(attn)\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.bmm(attn, v)\n",
        "\n",
        "        return output, attn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tYZJzsfxp2bf"
      },
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param n_head: int, ヘッド数\n",
        "        :param d_model: int, 隠れ層の次元数\n",
        "        :param d_k: int, keyベクトルの次元数\n",
        "        :param d_v: int, valueベクトルの次元数\n",
        "        :param dropout: float, ドロップアウト率\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "\n",
        "        # 各ヘッドごとに異なる重みで線形変換を行うための重み\n",
        "        # nn.Parameterを使うことで、Moduleのパラメータとして登録できる. TFでは更新が必要な変数はtf.Variableでラップするのでわかりやすい\n",
        "        self.w_qs = nn.Parameter(torch.empty([n_head, d_model, d_k], dtype=torch.float))\n",
        "        self.w_ks = nn.Parameter(torch.empty([n_head, d_model, d_k], dtype=torch.float))\n",
        "        self.w_vs = nn.Parameter(torch.empty([n_head, d_model, d_v], dtype=torch.float))\n",
        "        # nn.init.xavier_normal_で重みの値を初期化\n",
        "        nn.init.xavier_normal_(self.w_qs)\n",
        "        nn.init.xavier_normal_(self.w_ks)\n",
        "        nn.init.xavier_normal_(self.w_vs)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(d_model)\n",
        "        self.layer_norm = nn.LayerNorm(d_model) # 各層においてバイアスを除く活性化関数への入力を平均０、分散１に正則化\n",
        "        self.proj = nn.Linear(n_head*d_v, d_model)  # 複数ヘッド分のAttentionの結果を元のサイズに写像するための線形層\n",
        "        # nn.init.xavier_normal_で重みの値を初期化\n",
        "        nn.init.xavier_normal_(self.proj.weight)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, q, k, v, attn_mask=None):\n",
        "        \"\"\"\n",
        "        :param q: torch.tensor, queryベクトル, \n",
        "            size=(batch_size, len_q, d_model)\n",
        "        :param k: torch.tensor, key, \n",
        "            size=(batch_size, len_k, d_model)\n",
        "        :param v: torch.tensor, valueベクトル, \n",
        "            size=(batch_size, len_v, d_model)\n",
        "        :param attn_mask: torch.tensor, Attentionに適用するマスク, \n",
        "            size=(batch_size, len_q, len_k)\n",
        "        :return outputs: 出力ベクトル, \n",
        "            size=(batch_size, len_q, d_model)\n",
        "        :return attns: Attention\n",
        "            size=(n_head*batch_size, len_q, len_k)\n",
        "            \n",
        "        \"\"\"\n",
        "        d_k, d_v = self.d_k, self.d_v\n",
        "        n_head = self.n_head\n",
        "\n",
        "        # residual connectionのための入力 出力に入力をそのまま加算する\n",
        "        residual = q\n",
        "\n",
        "        batch_size, len_q, d_model = q.size()\n",
        "        batch_size, len_k, d_model = k.size()\n",
        "        batch_size, len_v, d_model = v.size()\n",
        "\n",
        "        # 複数ヘッド化\n",
        "        # torch.repeat または .repeatで指定したdimに沿って同じテンソルを作成\n",
        "        q_s = q.repeat(n_head, 1, 1) # (n_head*batch_size, len_q, d_model)\n",
        "        k_s = k.repeat(n_head, 1, 1) # (n_head*batch_size, len_k, d_model)\n",
        "        v_s = v.repeat(n_head, 1, 1) # (n_head*batch_size, len_v, d_model)\n",
        "        # ヘッドごとに並列計算させるために、n_headをdim=0に、batch_sizeをdim=1に寄せる\n",
        "        q_s = q_s.view(n_head, -1, d_model) # (n_head, batch_size*len_q, d_model)\n",
        "        k_s = k_s.view(n_head, -1, d_model) # (n_head, batch_size*len_k, d_model)\n",
        "        v_s = v_s.view(n_head, -1, d_model) # (n_head, batch_size*len_v, d_model)\n",
        "\n",
        "        # 各ヘッドで線形変換を並列計算(p16左側`Linear`)\n",
        "        q_s = torch.bmm(q_s, self.w_qs)  # (n_head, batch_size*len_q, d_k)\n",
        "        k_s = torch.bmm(k_s, self.w_ks)  # (n_head, batch_size*len_k, d_k)\n",
        "        v_s = torch.bmm(v_s, self.w_vs)  # (n_head, batch_size*len_v, d_v)\n",
        "        # Attentionは各バッチ各ヘッドごとに計算させるためにbatch_sizeをdim=0に寄せる\n",
        "        q_s = q_s.view(-1, len_q, d_k)   # (n_head*batch_size, len_q, d_k)\n",
        "        k_s = k_s.view(-1, len_k, d_k)   # (n_head*batch_size, len_k, d_k)\n",
        "        v_s = v_s.view(-1, len_v, d_v)   # (n_head*batch_size, len_v, d_v)\n",
        "\n",
        "        # Attentionを計算(p16.左側`Scaled Dot-Product Attention * h`)\n",
        "        outputs, attns = self.attention(q_s, k_s, v_s, attn_mask=attn_mask.repeat(n_head, 1, 1))\n",
        "\n",
        "        # 各ヘッドの結果を連結(p16左側`Concat`)\n",
        "        # torch.splitでbatch_sizeごとのn_head個のテンソルに分割\n",
        "        outputs = torch.split(outputs, batch_size, dim=0)  # (batch_size, len_q, d_model) * n_head\n",
        "        # dim=-1で連結\n",
        "        outputs = torch.cat(outputs, dim=-1)  # (batch_size, len_q, d_model*n_head)\n",
        "\n",
        "        # residual connectionのために元の大きさに写像(p16左側`Linear`)\n",
        "        outputs = self.proj(outputs)  # (batch_size, len_q, d_model)\n",
        "        outputs = self.dropout(outputs)\n",
        "        outputs = self.layer_norm(outputs + residual)\n",
        "\n",
        "        return outputs, attns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Eor7Q0Zp2bo"
      },
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    :param d_hid: int, 隠れ層1層目の次元数\n",
        "    :param d_inner_hid: int, 隠れ層2層目の次元数\n",
        "    :param dropout: float, ドロップアウト率\n",
        "    \"\"\"\n",
        "    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        # window size 1のconv層を定義することでPosition wiseな全結合層を実現する.\n",
        "        self.w_1 = nn.Conv1d(d_hid, d_inner_hid, 1)\n",
        "        self.w_2 = nn.Conv1d(d_inner_hid, d_hid, 1)\n",
        "        self.layer_norm = nn.LayerNorm(d_hid)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: torch.tensor,\n",
        "            size=(batch_size, max_length, d_hid)\n",
        "        :return: torch.tensor,\n",
        "            size=(batch_size, max_length, d_hid) \n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        output = self.relu(self.w_1(x.transpose(1, 2)))\n",
        "        output = self.w_2(output).transpose(2, 1)\n",
        "        output = self.dropout(output)\n",
        "        return self.layer_norm(output + residual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "amN70mbwp2bv"
      },
      "cell_type": "code",
      "source": [
        "def get_attn_padding_mask(seq_q, seq_k):\n",
        "    \"\"\"\n",
        "    keyのPADに対するattentionを0にするためのマスクを作成する\n",
        "    :param seq_q: tensor, queryの系列, size=(batch_size, len_q)\n",
        "    :param seq_k: tensor, keyの系列, size=(batch_size, len_k)\n",
        "    :return pad_attn_mask: tensor, size=(batch_size, len_q, len_k)\n",
        "    \"\"\"\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    pad_attn_mask = seq_k.data.eq(PAD).unsqueeze(1)   # (N, 1, len_k) PAD以外のidを全て0にする\n",
        "    pad_attn_mask = pad_attn_mask.expand(batch_size, len_q, len_k) # (N, len_q, len_k)\n",
        "    return pad_attn_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IW8zvfLcp2b3"
      },
      "cell_type": "code",
      "source": [
        "def get_attn_subsequent_mask(seq):\n",
        "    \"\"\"\n",
        "    未来の情報に対するattentionを0にするためのマスクを作成する\n",
        "    :param seq: tensor, size=(batch_size, length)\n",
        "    :return subsequent_mask: tensor, size=(batch_size, length, length)\n",
        "    \"\"\"\n",
        "    attn_shape = (seq.size(1), seq.size(1))\n",
        "    # 上三角行列(diagonal=1: 対角線より上が1で下が0)\n",
        "    subsequent_mask = torch.triu(torch.ones(attn_shape, dtype=torch.uint8, device=device), diagonal=1)\n",
        "    subsequent_mask = subsequent_mask.repeat(seq.size(0), 1, 1)\n",
        "    return subsequent_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LtpNJZXxp2cA"
      },
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"Encoderのブロックのクラス\"\"\"\n",
        "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param d_model: int, 隠れ層の次元数\n",
        "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
        "        :param n_head: int,　ヘッド数\n",
        "        :param d_k: int, keyベクトルの次元数\n",
        "        :param d_v: int, valueベクトルの次元数\n",
        "        :param dropout: float, ドロップアウト率\n",
        "        \"\"\"\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        # Encoder内のSelf-Attention\n",
        "        self.slf_attn = MultiHeadAttention(\n",
        "            n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "        # Postionwise FFN\n",
        "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
        "\n",
        "    def forward(self, enc_input, slf_attn_mask=None):\n",
        "        \"\"\"\n",
        "        :param enc_input: tensor, Encoderの入力, \n",
        "            size=(batch_size, max_length, d_model)\n",
        "        :param slf_attn_mask: tensor, Self Attentionの行列にかけるマスク, \n",
        "            size=(batch_size, len_q, len_k)\n",
        "        :return enc_output: tensor, Encoderの出力, \n",
        "            size=(batch_size, max_length, d_model)\n",
        "        :return enc_slf_attn: tensor, EncoderのSelf Attentionの行列, \n",
        "            size=(n_head*batch_size, len_q, len_k)\n",
        "        \"\"\"\n",
        "        # Self-Attentionのquery, key, valueにはすべてEncoderの入力（enc_input）が入る\n",
        "        enc_output, enc_slf_attn = self.slf_attn(\n",
        "            enc_input, enc_input, enc_input, attn_mask=slf_attn_mask)\n",
        "        enc_output = self.pos_ffn(enc_output)\n",
        "        return enc_output, enc_slf_attn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MHrEiFeEp2cF"
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"EncoderLayerブロックからなるEncoderのクラス\"\"\"\n",
        "    def __init__(\n",
        "            self, n_src_vocab, max_length, n_layers=6, n_head=8, d_k=64, d_v=64,\n",
        "            d_word_vec=512, d_model=512, d_inner_hid=1024, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param n_src_vocab: int, 入力言語の語彙数\n",
        "        :param max_length: int, 最大系列長\n",
        "        :param n_layers: int, レイヤー数\n",
        "        :param n_head: int,　ヘッド数\n",
        "        :param d_k: int, keyベクトルの次元数\n",
        "        :param d_v: int, valueベクトルの次元数\n",
        "        :param d_word_vec: int, 単語の埋め込みの次元数\n",
        "        :param d_model: int, 隠れ層の次元数\n",
        "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
        "        :param dropout: float, ドロップアウト率        \n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        n_position = max_length + 1\n",
        "        self.max_length = max_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Positional Encodingを用いたEmbedding\n",
        "        self.position_enc = nn.Embedding(n_position, d_word_vec, padding_idx=PAD)\n",
        "        self.position_enc.weight.data = position_encoding_init(n_position, d_word_vec)\n",
        "\n",
        "        # 一般的なEmbedding\n",
        "        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=PAD)\n",
        "\n",
        "        # EncoderLayerをn_layers個積み重ねる\n",
        "        self.layer_stack = nn.ModuleList([\n",
        "            EncoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout=dropout)\n",
        "            for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, src_seq, src_pos):\n",
        "        \"\"\"\n",
        "        :param src_seq: tensor, 入力系列, \n",
        "            size=(batch_size, max_length)\n",
        "        :param src_pos: tensor, 入力系列の各単語の位置情報,\n",
        "            size=(batch_size, max_length)\n",
        "        :return enc_output: tensor, Encoderの最終出力, \n",
        "            size=(batch_size, max_length, d_model)\n",
        "        :return enc_slf_attns: list, EncoderのSelf Attentionの行列のリスト\n",
        "        \"\"\"\n",
        "        # 一般的な単語のEmbeddingを行う\n",
        "        enc_input = self.src_word_emb(src_seq)\n",
        "        # Positional EncodingのEmbeddingを加算する\n",
        "        enc_input += self.position_enc(src_pos)\n",
        "\n",
        "        enc_slf_attns = []\n",
        "        enc_output = enc_input\n",
        "        # key(=enc_input)のPADに対応する部分のみ1のマスクを作成\n",
        "        enc_slf_attn_mask = get_attn_padding_mask(src_seq, src_seq)\n",
        "\n",
        "        # n_layers個のEncoderLayerに入力を通す\n",
        "        for enc_layer in self.layer_stack:\n",
        "            enc_output, enc_slf_attn = enc_layer(\n",
        "                enc_output, slf_attn_mask=enc_slf_attn_mask)\n",
        "            enc_slf_attns += [enc_slf_attn]\n",
        "\n",
        "        return enc_output, enc_slf_attns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2WFWRsXYp2cJ"
      },
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"Decoderのブロックのクラス\"\"\"\n",
        "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param d_model: int, 隠れ層の次元数\n",
        "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
        "        :param n_head: int,　ヘッド数\n",
        "        :param d_k: int, keyベクトルの次元数\n",
        "        :param d_v: int, valueベクトルの次元数\n",
        "        :param dropout: float, ドロップアウト率\n",
        "        \"\"\"\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        # Decoder内のSelf-Attention\n",
        "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "        # Encoder-Decoder間のSource-Target Attention\n",
        "        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "        # Positionwise FFN\n",
        "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
        "\n",
        "    def forward(self, dec_input, enc_output, slf_attn_mask=None, dec_enc_attn_mask=None):\n",
        "        \"\"\"\n",
        "        :param dec_input: tensor, Decoderの入力, \n",
        "            size=(batch_size, max_length, d_model)\n",
        "        :param enc_output: tensor, Encoderの出力, \n",
        "            size=(batch_size, max_length, d_model)\n",
        "        :param slf_attn_mask: tensor, Self Attentionの行列にかけるマスク, \n",
        "            size=(batch_size, len_q, len_k)\n",
        "        :param dec_enc_attn_mask: tensor, Soutce-Target Attentionの行列にかけるマスク, \n",
        "            size=(batch_size, len_q, len_k)\n",
        "        :return dec_output: tensor, Decoderの出力, \n",
        "            size=(batch_size, max_length, d_model)\n",
        "        :return dec_slf_attn: tensor, DecoderのSelf Attentionの行列, \n",
        "            size=(n_head*batch_size, len_q, len_k)\n",
        "        :return dec_enc_attn: tensor, DecoderのSoutce-Target Attentionの行列, \n",
        "            size=(n_head*batch_size, len_q, len_k)\n",
        "        \"\"\"\n",
        "        # Self-Attentionのquery, key, valueにはすべてDecoderの入力（dec_input）が入る\n",
        "        dec_output, dec_slf_attn = self.slf_attn(\n",
        "            dec_input, dec_input, dec_input, attn_mask=slf_attn_mask)\n",
        "        # Source-Target-AttentionのqueryにはDecoderの出力(dec_output), key, valueにはEncoderの出力（enc_output）が入る\n",
        "        dec_output, dec_enc_attn = self.enc_attn(\n",
        "            dec_output, enc_output, enc_output, attn_mask=dec_enc_attn_mask)\n",
        "        dec_output = self.pos_ffn(dec_output)\n",
        "\n",
        "        return dec_output, dec_slf_attn, dec_enc_attn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cnBteVrjp2cL"
      },
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"DecoderLayerブロックからなるDecoderのクラス\"\"\"\n",
        "    def __init__(\n",
        "            self, n_tgt_vocab, max_length, n_layers=6, n_head=8, d_k=64, d_v=64,\n",
        "            d_word_vec=512, d_model=512, d_inner_hid=1024, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param n_tgt_vocab: int, 出力言語の語彙数\n",
        "        :param max_length: int, 最大系列長\n",
        "        :param n_layers: int, レイヤー数\n",
        "        :param n_head: int,　ヘッド数\n",
        "        :param d_k: int, keyベクトルの次元数\n",
        "        :param d_v: int, valueベクトルの次元数\n",
        "        :param d_word_vec: int, 単語の埋め込みの次元数\n",
        "        :param d_model: int, 隠れ層の次元数\n",
        "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
        "        :param dropout: float, ドロップアウト率        \n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "        n_position = max_length + 1\n",
        "        self.max_length = max_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Positional Encodingを用いたEmbedding\n",
        "        self.position_enc = nn.Embedding(\n",
        "            n_position, d_word_vec, padding_idx=PAD)\n",
        "        self.position_enc.weight.data = position_encoding_init(n_position, d_word_vec)\n",
        "\n",
        "        # 一般的なEmbedding\n",
        "        self.tgt_word_emb = nn.Embedding(\n",
        "            n_tgt_vocab, d_word_vec, padding_idx=PAD)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # DecoderLayerをn_layers個積み重ねる\n",
        "        self.layer_stack = nn.ModuleList([\n",
        "            DecoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout=dropout)\n",
        "            for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, tgt_seq, tgt_pos, src_seq, enc_output):\n",
        "        \"\"\"\n",
        "        :param tgt_seq: tensor, 出力系列, \n",
        "            size=(batch_size, max_length)\n",
        "        :param tgt_pos: tensor, 出力系列の各単語の位置情報,\n",
        "            size=(batch_size, max_length)\n",
        "        :param src_seq: tensor, 入力系列, \n",
        "            size=(batch_size, n_src_vocab)\n",
        "        :param enc_output: tensor, Encoderの出力, \n",
        "            size=(batch_size, max_length, d_model)\n",
        "        :return dec_output: tensor, Decoderの最終出力, \n",
        "            size=(batch_size, max_length, d_model)\n",
        "        :return dec_slf_attns: list, DecoderのSelf Attentionの行列のリスト \n",
        "        :return dec_slf_attns: list, DecoderのSelf Attentionの行列のリスト\n",
        "        \"\"\"\n",
        "        # 一般的な単語のEmbeddingを行う\n",
        "        dec_input = self.tgt_word_emb(tgt_seq)\n",
        "        # Positional EncodingのEmbeddingを加算する\n",
        "        dec_input += self.position_enc(tgt_pos)\n",
        "\n",
        "        # Self-Attention用のマスクを作成\n",
        "        # key(=dec_input)のPADに対応する部分が1のマスクと、queryから見たkeyの未来の情報に対応する部分が1のマスクのORをとる\n",
        "        dec_slf_attn_pad_mask = get_attn_padding_mask(tgt_seq, tgt_seq)  # (N, max_length, max_length)\n",
        "        dec_slf_attn_sub_mask = get_attn_subsequent_mask(tgt_seq)  # (N, max_length, max_length)\n",
        "        dec_slf_attn_mask = torch.gt(dec_slf_attn_pad_mask + dec_slf_attn_sub_mask, 0)  # ORをとる\n",
        "\n",
        "        # key(=dec_input)のPADに対応する部分のみ1のマスクを作成\n",
        "        dec_enc_attn_pad_mask = get_attn_padding_mask(tgt_seq, src_seq)  # (N, max_length, max_length)\n",
        "\n",
        "        dec_slf_attns, dec_enc_attns = [], []\n",
        "\n",
        "        dec_output = dec_input\n",
        "        # n_layers個のDecoderLayerに入力を通す\n",
        "        for dec_layer in self.layer_stack:\n",
        "            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n",
        "                dec_output, enc_output,\n",
        "                slf_attn_mask=dec_slf_attn_mask,\n",
        "                dec_enc_attn_mask=dec_enc_attn_pad_mask)\n",
        "\n",
        "            dec_slf_attns += [dec_slf_attn]\n",
        "            dec_enc_attns += [dec_enc_attn]\n",
        "\n",
        "        return dec_output, dec_slf_attns, dec_enc_attns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uGZTXI2xp2cN"
      },
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"Transformerのモデル全体のクラス\"\"\"\n",
        "    def __init__(\n",
        "            self, n_src_vocab, n_tgt_vocab, max_length, n_layers=6, n_head=8,\n",
        "            d_word_vec=512, d_model=512, d_inner_hid=1024, d_k=64, d_v=64,\n",
        "            dropout=0.1, proj_share_weight=True):\n",
        "        \"\"\"\n",
        "        :param n_src_vocab: int, 入力言語の語彙数\n",
        "        :param n_tgt_vocab: int, 出力言語の語彙数\n",
        "        :param max_length: int, 最大系列長\n",
        "        :param n_layers: int, レイヤー数\n",
        "        :param n_head: int,　ヘッド数\n",
        "        :param d_k: int, keyベクトルの次元数\n",
        "        :param d_v: int, valueベクトルの次元数\n",
        "        :param d_word_vec: int, 単語の埋め込みの次元数\n",
        "        :param d_model: int, 隠れ層の次元数\n",
        "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
        "        :param dropout: float, ドロップアウト率        \n",
        "        :param proj_share_weight: bool, 出力言語の単語のEmbeddingと出力の写像で重みを共有する        \n",
        "        \"\"\"\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(\n",
        "            n_src_vocab, max_length, n_layers=n_layers, n_head=n_head,\n",
        "            d_word_vec=d_word_vec, d_model=d_model,\n",
        "            d_inner_hid=d_inner_hid, dropout=dropout)\n",
        "        self.decoder = Decoder(\n",
        "            n_tgt_vocab, max_length, n_layers=n_layers, n_head=n_head,\n",
        "            d_word_vec=d_word_vec, d_model=d_model,\n",
        "            d_inner_hid=d_inner_hid, dropout=dropout)\n",
        "        self.tgt_word_proj = nn.Linear(d_model, n_tgt_vocab, bias=False)\n",
        "        nn.init.xavier_normal_(self.tgt_word_proj.weight)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        assert d_model == d_word_vec  # 各モジュールの出力のサイズは揃える\n",
        "\n",
        "        if proj_share_weight:\n",
        "            # 出力言語の単語のEmbeddingと出力の写像で重みを共有する\n",
        "            assert d_model == d_word_vec\n",
        "            self.tgt_word_proj.weight = self.decoder.tgt_word_emb.weight\n",
        "\n",
        "    def get_trainable_parameters(self):\n",
        "        # Positional Encoding以外のパラメータを更新する\n",
        "        enc_freezed_param_ids = set(map(id, self.encoder.position_enc.parameters()))\n",
        "        dec_freezed_param_ids = set(map(id, self.decoder.position_enc.parameters()))\n",
        "        freezed_param_ids = enc_freezed_param_ids | dec_freezed_param_ids\n",
        "        return (p for p in self.parameters() if id(p) not in freezed_param_ids)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_seq, src_pos = src\n",
        "        tgt_seq, tgt_pos = tgt\n",
        "\n",
        "        src_seq = src_seq[:, 1:]\n",
        "        src_pos = src_pos[:, 1:]\n",
        "        tgt_seq = tgt_seq[:, :-1]\n",
        "        tgt_pos = tgt_pos[:, :-1]\n",
        "\n",
        "        enc_output, *_ = self.encoder(src_seq, src_pos)\n",
        "        dec_output, *_ = self.decoder(tgt_seq, tgt_pos, src_seq, enc_output)\n",
        "        seq_logit = self.tgt_word_proj(dec_output)\n",
        "\n",
        "        return seq_logit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qRsUUhMup2cR"
      },
      "cell_type": "code",
      "source": [
        "def compute_loss(batch_X, batch_Y, model, criterion, optimizer=None, is_train=True):\n",
        "    # バッチの損失を計算\n",
        "    model.train(is_train)\n",
        "    \n",
        "    pred_Y = model(batch_X, batch_Y)\n",
        "    gold = batch_Y[0][:, 1:].contiguous()\n",
        "#     gold = batch_Y[0].contiguous()\n",
        "    loss = criterion(pred_Y.view(-1, pred_Y.size(2)), gold.view(-1))\n",
        "\n",
        "    if is_train:  # 訓練時はパラメータを更新\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    gold = gold.data.cpu().numpy().tolist()\n",
        "    pred = pred_Y.max(dim=-1)[1].data.cpu().numpy().tolist()\n",
        "\n",
        "    return loss.item(), gold, pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gVeXwSp3p2cW"
      },
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 20\n",
        "batch_size = 64\n",
        "num_epochs = 15\n",
        "lr = 0.001\n",
        "ckpt_path = 'transformer.pth'\n",
        "max_length = MAX_LENGTH + 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B5UNWBmSp2cY"
      },
      "cell_type": "code",
      "source": [
        "model_args = {\n",
        "    'n_src_vocab': vocab_size_X,\n",
        "    'n_tgt_vocab': vocab_size_Y,\n",
        "    'max_length': max_length,\n",
        "    'proj_share_weight': True,\n",
        "    'd_k': 32,\n",
        "    'd_v': 32,\n",
        "    'd_model': 128,\n",
        "    'd_word_vec': 128,\n",
        "    'd_inner_hid': 256,\n",
        "    'n_layers': 3,\n",
        "    'n_head': 6,\n",
        "    'dropout': 0.1,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4grdni3dp2ca",
        "outputId": "75b422ef-1a31-47fd-8971-62ef42f41565",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# DataLoaderやモデルを定義\n",
        "train_dataloader = DataLoader(\n",
        "    train_X, train_Y, batch_size\n",
        "    )\n",
        "valid_dataloader = DataLoader(\n",
        "    valid_X, valid_Y, batch_size, \n",
        "    shuffle=False\n",
        "    )\n",
        "\n",
        "model = Transformer(**model_args).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.get_trainable_parameters(), lr=lr)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD, size_average=False).to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "SnHO46bip2cd"
      },
      "cell_type": "code",
      "source": [
        "def calc_bleu(refs, hyps):\n",
        "    \"\"\"\n",
        "    BLEUスコアを計算する関数\n",
        "    :param refs: list, 参照訳。単語のリストのリスト (例： [['I', 'have', 'a', 'pen'], ...])\n",
        "    :param hyps: list, モデルの生成した訳。単語のリストのリスト (例： [['I', 'have', 'a', 'pen'], ...])\n",
        "    :return: float, BLEUスコア(0~100)\n",
        "    \"\"\"\n",
        "    refs = [[ref[:ref.index(EOS)]] for ref in refs]\n",
        "    hyps = [hyp[:hyp.index(EOS)] if EOS in hyp else hyp for hyp in hyps]\n",
        "    return 100 * bleu_score.corpus_bleu(refs, hyps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v5jKt9aop2cf",
        "outputId": "bb630675-a95a-4c90-9179-c7f0a9e3b63b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# 訓練\n",
        "best_valid_bleu = 0.\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    start = time.time()\n",
        "    train_loss = 0.\n",
        "    train_refs = []\n",
        "    train_hyps = []\n",
        "    valid_loss = 0.\n",
        "    valid_refs = []\n",
        "    valid_hyps = []\n",
        "    # train\n",
        "    for batch in train_dataloader:\n",
        "        batch_X, batch_Y = batch\n",
        "        loss, gold, pred = compute_loss(\n",
        "            batch_X, batch_Y, model, criterion, optimizer, is_train=True\n",
        "            )\n",
        "        train_loss += loss\n",
        "        train_refs += gold\n",
        "        train_hyps += pred\n",
        "    # valid\n",
        "    for batch in valid_dataloader:\n",
        "        batch_X, batch_Y = batch\n",
        "        loss, gold, pred = compute_loss(\n",
        "            batch_X, batch_Y, model, criterion, is_train=False\n",
        "            )\n",
        "        valid_loss += loss\n",
        "        valid_refs += gold\n",
        "        valid_hyps += pred\n",
        "    # 損失をサンプル数で割って正規化\n",
        "    train_loss /= len(train_dataloader.data) \n",
        "    valid_loss /= len(valid_dataloader.data) \n",
        "    # BLEUを計算\n",
        "    train_bleu = calc_bleu(train_refs, train_hyps)\n",
        "    valid_bleu = calc_bleu(valid_refs, valid_hyps)\n",
        "\n",
        "    # validationデータでBLEUが改善した場合にはモデルを保存\n",
        "    if valid_bleu > best_valid_bleu:\n",
        "        ckpt = model.state_dict()\n",
        "        torch.save(ckpt, ckpt_path)\n",
        "        best_valid_bleu = valid_bleu\n",
        "\n",
        "    elapsed_time = (time.time()-start) / 60\n",
        "    print('Epoch {} [{:.1f}min]: train_loss: {:5.2f}  train_bleu: {:2.2f}  valid_loss: {:5.2f}  valid_bleu: {:2.2f}'.format(\n",
        "            epoch, elapsed_time, train_loss, train_bleu, valid_loss, valid_bleu))\n",
        "    print('-'*80)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 [0.6min]: train_loss: 77.34  train_bleu: 4.77  valid_loss: 41.31  valid_bleu: 11.27\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 [0.6min]: train_loss: 39.27  train_bleu: 12.31  valid_loss: 32.12  valid_bleu: 17.52\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 [0.6min]: train_loss: 31.83  train_bleu: 18.21  valid_loss: 28.06  valid_bleu: 21.84\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 [0.6min]: train_loss: 28.11  train_bleu: 21.84  valid_loss: 25.58  valid_bleu: 25.02\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5 [0.6min]: train_loss: 25.67  train_bleu: 24.76  valid_loss: 24.24  valid_bleu: 27.06\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6 [0.7min]: train_loss: 23.86  train_bleu: 26.99  valid_loss: 22.98  valid_bleu: 29.12\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7 [0.6min]: train_loss: 22.43  train_bleu: 28.63  valid_loss: 22.06  valid_bleu: 30.57\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8 [0.7min]: train_loss: 21.25  train_bleu: 30.38  valid_loss: 21.42  valid_bleu: 31.39\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9 [0.7min]: train_loss: 20.24  train_bleu: 31.67  valid_loss: 20.91  valid_bleu: 31.92\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10 [0.7min]: train_loss: 19.34  train_bleu: 32.96  valid_loss: 20.42  valid_bleu: 32.85\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 11 [0.6min]: train_loss: 18.59  train_bleu: 34.00  valid_loss: 20.03  valid_bleu: 33.99\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 12 [0.6min]: train_loss: 17.85  train_bleu: 35.18  valid_loss: 19.84  valid_bleu: 33.90\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 13 [0.6min]: train_loss: 17.24  train_bleu: 36.08  valid_loss: 19.35  valid_bleu: 34.87\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 14 [0.7min]: train_loss: 16.63  train_bleu: 37.02  valid_loss: 19.13  valid_bleu: 35.26\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 15 [0.7min]: train_loss: 16.11  train_bleu: 37.94  valid_loss: 19.16  valid_bleu: 35.40\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}